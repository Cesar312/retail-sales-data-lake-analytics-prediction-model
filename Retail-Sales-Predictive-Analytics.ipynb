{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03fd89a5-5b07-4188-825a-525dbb44f7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b814e5ee-fdd3-4472-ba15-fac2af330ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc0d8b4-9866-4c66-a7a9-5682b89ab9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, col \n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5bc2930-025b-492c-b6b3-8936c6b905e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a2feba9-ff3f-4415-a01c-59661f48ba3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initiate Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesPredictionWithMLflow\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd107914-183a-46f4-941e-a195236756b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/salesdata/published\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e92c8a-b8b0-4096-9db5-e9a12e3b045c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+\n|       City|ReportYear|ReportMonth|TotalSales|\n+-----------+----------+-----------+----------+\n|Los Angeles|      2019|         12| 684044.84|\n|     Boston|      2019|         12| 509599.16|\n|     Austin|      2019|          4| 172683.59|\n|     Austin|      2019|         10| 203196.12|\n|     Austin|      2019|         12| 233777.09|\n|     Dallas|      2019|          4| 251360.48|\n|     Dallas|      2019|         10| 323135.60|\n|   Portland|      2019|         12| 303714.11|\n|   Portland|      2019|          4| 239978.12|\n|   Portland|      2019|         10| 254100.86|\n+-----------+----------+-----------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "aggregated_df = (\n",
    "    sales_df\n",
    "    .groupBy(\"City\", \"ReportYear\", \"ReportMonth\")\n",
    "    .agg(spark_sum(col(\"Price\") * col(\"Quantity\")).alias(\"TotalSales\"))\n",
    ")\n",
    "\n",
    "aggregated_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ce1095b-9fdd-48ec-8225-2f7b46c58805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33dd567-61ec-4203-83f2-514b6e8ddf18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Index the City column (convert the city names to numeric values)\n",
    "city_indexer = StringIndexer(inputCol=\"City\", outputCol=\"CityIndex\")\n",
    "\n",
    "# One-Hot-Encode (OHE)the indexed city\n",
    "city_encoder = OneHotEncoder(inputCols=[\"CityIndex\"], outputCols=[\"CityOHE\"])\n",
    "\n",
    "# Assemble all the features into a single vector\n",
    "feature_assembler = VectorAssembler(\n",
    "  inputCols=[\"CityOHE\", \"ReportYear\", \"ReportMonth\"],\n",
    "  outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29196e3e-52cc-4fd4-94f7-1cb99b6bd734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Training and Tracking with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9daeee3a-9a2c-47b5-816d-b7aea1322cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data 80/20 for training and testing\n",
    "train_df, test_df = aggregated_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e538fe6a-b1d9-422f-9e9e-f6946cf48920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Mlflow autologging\n",
    "mlflow.pyspark.ml.autolog()\n",
    "\n",
    "# Define Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"TotalSales\", predictionCol=\"prediction\")\n",
    "\n",
    "# Create the ML Pipeline\n",
    "pipeline = Pipeline(stages=[city_indexer, city_encoder, feature_assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151e3992-e0bd-4469-b809-a8dac277aeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 15:39:41 INFO mlflow.tracking.fluent: Experiment with name '/Users/cesar-martinez1@northwestern.edu/sales-prediction-experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: <Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/3642522158388303', creation_time=1742225981386, experiment_id='3642522158388303', last_update_time=1742225981386, lifecycle_stage='active', name='/Users/cesar-martinez1@northwestern.edu/sales-prediction-experiment', tags={'mlflow.experiment.sourceName': '/Users/cesar-martinez1@northwestern.edu/sales-prediction-experiment',\n 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n 'mlflow.ownerEmail': 'cesar-martinez1@northwestern.edu',\n 'mlflow.ownerId': '3846535268250455'}>"
     ]
    }
   ],
   "source": [
    "# Start the MLflow experiment\n",
    "experiment_name = \"/Users/cesar-martinez1@northwestern.edu/sales-prediction-experiment\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0468cf-0248-49e1-a36c-b9269417de7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 15:52:11 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('TotalSales', DecimalType(31,2), True)]. Model signature is not logged.\n2025/03/17 15:52:31 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/03/17 15:53:26 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\"\n2025/03/17 15:54:01 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 69265.94\nMAE: 60073.23\nR²: 0.84\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Predict on the test dataset\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=\"TotalSales\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R-squared\", r2)\n",
    "\n",
    "    # Save model in MLflow\n",
    "    mlflow.spark.log_model(model, \"sales_prediction_model\")\n",
    "\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R²: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514ae53a-73e2-492b-9ec4-e03d748bbb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+----------+------------------+\n|   City|ReportYear|ReportMonth|TotalSales|        prediction|\n+-------+----------+-----------+----------+------------------+\n|Atlanta|      2019|          3| 231905.38|173040.52566999197|\n|Atlanta|      2019|          7| 211766.47|232318.95373147726|\n|Atlanta|      2019|          9| 171278.89| 261958.1677621603|\n| Austin|      2019|          2| 108787.40|130581.04956585169|\n| Austin|      2019|          8| 125713.61|219498.69165802002|\n+-------+----------+-----------+----------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show a few prediction results\n",
    "predictions.select(\"City\", \"ReportYear\", \"ReportMonth\", \"TotalSales\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fbe9ea2-7900-49df-87cb-e9c44e1b029d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Make Future Predictions with Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4e15fc-153d-4ecb-b375-92baca3813ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+\n|         City|ReportYear|ReportMonth|\n+-------------+----------+-----------+\n|New York City|      2020|          2|\n|San Francisco|      2020|          2|\n|  Los Angeles|      2020|          2|\n+-------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Prepare new prediction data\n",
    "new_data = spark.createDataFrame([\n",
    "    (\"New York City\", 2020, 2),\n",
    "    (\"San Francisco\", 2020, 2),\n",
    "    (\"Los Angeles\", 2020, 2)\n",
    "], [\"City\", \"ReportYear\", \"ReportMonth\"])\n",
    "\n",
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ef7e9a-cf23-495b-aaf4-3c33d8493a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 17:29:08 INFO mlflow.spark: 'runs:/9065070942034d3089dc9aab3eeb5094/sales_prediction_model' resolved as 'dbfs:/databricks/mlflow-tracking/3642522158388303/9065070942034d3089dc9aab3eeb5094/artifacts/sales_prediction_model'\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model from MLflow\n",
    "logged_model = 'runs:/9065070942034d3089dc9aab3eeb5094/sales_prediction_model'\n",
    "loaded_model = mlflow.spark.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3554cb94-4e5a-48c5-9c20-25a7967ba5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+------------------+\n|         City|ReportYear|ReportMonth|        prediction|\n+-------------+----------+-----------+------------------+\n|New York City|      2020|          2|  72888.5575813055|\n|San Francisco|      2020|          2|334278.54314494133|\n|  Los Angeles|      2020|          2|128456.74791198969|\n+-------------+----------+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform inference via model.transform()\n",
    "new_predictions = loaded_model.transform(new_data)\n",
    "new_predictions.select(\"City\", \"ReportYear\", \"ReportMonth\", \"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail-Sales-Prediction-Model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
